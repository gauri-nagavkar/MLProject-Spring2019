{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "# Emotion analysis from EEG data\n",
    "\n",
    "#### Barr Morgenstein Gauri Nagavkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of 32 .dat files, corresponding to 32 participants.\n",
    "\n",
    "Every .dat file has EEG data and Emotion labels data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_list = ['s01.dat', 's02.dat', 's03.dat', 's04.dat', 's05.dat', 's06.dat', 's07.dat', 's08.dat', 's09.dat', 's10.dat',\n",
    "            's11.dat', 's12.dat', 's13.dat', 's14.dat', 's15.dat', 's16.dat', 's17.dat', 's18.dat', 's19.dat', 's20.dat',\n",
    "            's21.dat', 's22.dat', 's23.dat', 's24.dat', 's25.dat', 's26.dat', 's27.dat', 's28.dat', 's29.dat', 's30.dat',\n",
    "            's31.dat', 's32.dat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract data from .dat files in the form of X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_to_dict(dat_file):\n",
    "    infile= open(dat_file, 'rb')\n",
    "    data_temp= pickle.load(infile, encoding='latin1')\n",
    "    infile.close()\n",
    "    \n",
    "    X_temp= data_temp[\"data\"]\n",
    "    X_temp= X_temp[:, :-8]\n",
    "    y_temp= data_temp[\"labels\"]  \n",
    "    \n",
    "    return X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create arrays X (EEG data) and y (emotion labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= dat_to_dict(dat_list[0])\n",
    "\n",
    "for i in range(1,32):\n",
    "    X1, y1= dat_to_dict(dat_list[i])\n",
    "    \n",
    "    X= np.vstack((X, X1))\n",
    "    y= np.vstack((y, y1))\n",
    "    \n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale X and y\n",
    "\n",
    "Remove mean and divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean= np.mean(y, axis=0)\n",
    "y_n= (y-y_mean)/np.std(y)\n",
    "\n",
    "y_n= np.float32(y_n)\n",
    "\n",
    "\n",
    "X_mean= np.mean(X, axis=0)\n",
    "X_n= (X-X_mean)/np.std(X)\n",
    "\n",
    "X_n= np.float32(X_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape normalized X and convert it to a 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp, nchan, nt = X_n.shape\n",
    "p= nchan*nt\n",
    "X_n = X_n.reshape((nsamp, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate y into its different label components\n",
    "\n",
    "Convert the y components into 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valence y\n",
    "y0 = y_n[:,0] \n",
    "\n",
    "y0= np.where((y0>=0.5),4, y0)\n",
    "y0= np.where((y0>= -0.5) & (y0<0),2, y0)\n",
    "y0= np.where((y0>= 0) & (y0<0.5),3, y0)\n",
    "y0= np.where((y0<0.5),1, y0)\n",
    "\n",
    "#Arousal y\n",
    "y1 = y_n[:,1] \n",
    "\n",
    "y1= np.where((y1>=0.5),4, y1)\n",
    "y1= np.where((y1>= -0.5) & (y1<0),2, y1)\n",
    "y1= np.where((y1>= 0) & (y1<0.5),3, y1)\n",
    "y1= np.where((y1<0.5),1, y1)\n",
    "\n",
    "#Dominance y\n",
    "y2 = y_n[:,2] \n",
    "\n",
    "y2= np.where((y2>=0.5),4, y2)\n",
    "y2= np.where((y2>= -0.5) & (y2<0),2, y2)\n",
    "y2= np.where((y2>= 0) & (y2<0.5),3, y2)\n",
    "y2= np.where((y2<0.5),1, y2)\n",
    "\n",
    "#Liking y\n",
    "y3 = y_n[:,3]\n",
    "\n",
    "y3= np.where((y3>=0.5),4, y3)\n",
    "y3= np.where((y3>= -0.5) & (y3<0),2, y3)\n",
    "y3= np.where((y3>= 0) & (y3<0.5),3, y3)\n",
    "y3= np.where((y3<0.5),1, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split normalized X and y into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0tr, X0ts, y0tr, y0ts = train_test_split(X_n, y0, test_size=0.33)\n",
    "X1tr, X1ts, y1tr, y1ts = train_test_split(X_n, y1, test_size=0.33)\n",
    "X2tr, X2ts, y2tr, y2ts = train_test_split(X_n, y2, test_size=0.33)\n",
    "X3tr, X3ts, y3tr, y3ts = train_test_split(X_n, y3, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA function for obtaining principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(X, ncomp):\n",
    "    pca= PCA(n_components= ncomp, svd_solver= 'randomized', whiten= True)\n",
    "    pca.fit(X)\n",
    "    Z= pca.transform(X)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svcc(Xtr, ytr, Xts, yts):\n",
    "    #clf = svm.linearSVC(mu;ti_class='ovr')\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    yhat= clf.predict(Xts)\n",
    "\n",
    "    acc = np.mean(yhat == yts)\n",
    "    \n",
    "    return yhat, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(Xtr, ytr, Xts, yts):\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    yhat= clf.predict(Xts)\n",
    "\n",
    "    acc = np.mean(yhat == yts)\n",
    "    \n",
    "    return yhat, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function for Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb(Xtr, ytr, Xts, yts):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(Xtr,ytr)\n",
    "    \n",
    "    yhat= clf.predict(Xts)\n",
    "    \n",
    "    acc = np.mean(yhat == yts)\n",
    "    \n",
    "    return yhat, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(300,400,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z0tr= pc(X0tr, ncomp)\n",
    "\n",
    "    Z0ts= pc(X0ts, ncomp)\n",
    "\n",
    "    y0hat, acc[icomp]= svcc(Z0tr, y0tr, Z0ts, y0ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Valence using SVM is 68.79 %, obtained using 300 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc\n",
    "print(f'The maximum accuracy for Valence using SVM is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(250, 300, 10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z0tr= pc(X0tr, ncomp)\n",
    "\n",
    "    Z0ts= pc(X0ts, ncomp)\n",
    "\n",
    "    y0hat, acc[icomp]= lr(Z0tr, y0tr, Z0ts, y0ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Valence using Logistic regression is 55.56 %, obtained using 255 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Valence using Logistic regression is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(350,400, 10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z0tr= pc(X0tr, ncomp)\n",
    "\n",
    "    Z0ts= pc(X0ts, ncomp)\n",
    "\n",
    "    y0hat, acc[icomp]= gnb(Z0tr, y0tr, Z0ts, y0ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Valence using Gaussian model is 48.23 %, obtained using 383 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Valence using Gaussian model is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(300,350,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z1tr= pc(X1tr, ncomp)\n",
    "\n",
    "    Z1ts= pc(X1ts, ncomp)\n",
    "\n",
    "    y1hat, acc[icomp]= svcc(Z1tr, y1tr, Z1ts, y1ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Arousal using SVM is 67.14 %, obtained using 300 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Arousal using SVM is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(65,120,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z1tr= pc(X1tr, ncomp)\n",
    "\n",
    "    Z1ts= pc(X1ts, ncomp)\n",
    "\n",
    "    y1hat, acc[icomp]= lr(Z1tr, y1tr, Z1ts, y1ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Arousal using Logistic regression is 63.36 %, obtained using 65 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Arousal using Logistic regression is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(250, 330, 10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z1tr= pc(X1tr, ncomp)\n",
    "\n",
    "    Z1ts= pc(X1ts, ncomp)\n",
    "\n",
    "    y1hat, acc[icomp]= gnb(Z1tr, y1tr, Z1ts, y1ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Arousal using Gaussian model is 53.19 %, obtained using 312 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Arousal using Gaussian model is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(60,120,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z2tr= pc(X2tr, ncomp)\n",
    "\n",
    "    Z2ts= pc(X2ts, ncomp)\n",
    "\n",
    "    y2hat, acc[icomp]= svcc(Z2tr, y2tr, Z2ts, y2ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Dominance using SVM is 65.96 %, obtained using 60 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Dominance using SVM is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(200,260,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z2tr= pc(X2tr, ncomp)\n",
    "\n",
    "    Z2ts= pc(X2ts, ncomp)\n",
    "\n",
    "    y2hat, acc[icomp]= lr(Z2tr, y2tr, Z2ts, y2ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Dominance using Logistic regression is 57.45 %, obtained using 220 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Dominance using Logistic regression is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(350,400, 10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z2tr= pc(X2tr, ncomp)\n",
    "\n",
    "    Z2ts= pc(X2ts, ncomp)\n",
    "\n",
    "    y2hat, acc[icomp]= gnb(Z2tr, y2tr, Z2ts, y2ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Dominance using Gaussian model is 51.34 %, obtained using 10 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Dominance using Gaussian model is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(10,70,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z3tr= pc(X3tr, ncomp)\n",
    "\n",
    "    Z3ts= pc(X3ts, ncomp)\n",
    "\n",
    "    y3hat, acc[icomp]= svcc(Z3tr, y3tr, Z3ts, y3ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Liking using SVM is 71.87 %, obtained using 10 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Liking using SVM is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(10,70,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z3tr= pc(X3tr, ncomp)\n",
    "\n",
    "    Z3ts= pc(X3ts, ncomp)\n",
    "\n",
    "    y3hat, acc[icomp]= lr(Z3tr, y3tr, Z3ts, y3ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Liking using Logisti is 71.63 %, obtained using 10 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Liking using Logistic Regression is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp_test= np.linspace(10,70,10, endpoint=True, dtype=int)\n",
    "num_nc = len(ncomp_test)\n",
    "\n",
    "acc = np.zeros((num_nc))\n",
    "\n",
    "# Loop over number of components to test\n",
    "for icomp, ncomp in enumerate(ncomp_test):\n",
    "\n",
    " # TODO: Fit the PCA on the scaled training data\n",
    "    Z3tr= pc(X3tr, ncomp)\n",
    "\n",
    "    Z3ts= pc(X3ts, ncomp)\n",
    "\n",
    "    y3hat, acc[icomp]= gnb(Z3tr, y3tr, Z3ts, y3ts)\n",
    "    \n",
    "    #print(icomp, ncomp, acc[icomp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for Liking using Gaussian model is 50.66 %, obtained using 70 principle components\n"
     ]
    }
   ],
   "source": [
    "imax = np.argmax(acc)\n",
    "print(f'The maximum accuracy for Liking using Gaussian model is {np.around(acc[imax]*100,2)} %, obtained using {ncomp_test[imax]} principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
